{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0307cacf-6275-4a62-87a9-1885e28e2fd3",
   "metadata": {},
   "source": [
    "# U.S. Geological Survey Class GW3099\n",
    "Advanced Modeling of Groundwater Flow (GW3099)\\\n",
    "Boise, Idaho\\\n",
    "September 16 - 20, 2024\n",
    "\n",
    "![title](../../images/ClassLocation.jpg)\n",
    "\n",
    "# Multi-process models in pywatershed\n",
    "*(Note that this notebook follows the notebook in the pywatershed repository [examples/01_multi-process_models.ipynb](https://github.com/EC-USGS/pywatershed/blob/develop/examples/01_multi-process_models.ipynb) but it deviates in some of the details covered.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e142cbc-9b4b-44d1-8991-f98a57243b08",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "In notebook [`step1_processes.ipynb`](step1_processes.ipynb), we looked at how individual Process representations work and are designed. In this notebook we learn how to put multiple `Processes` together into composite models using the `Model` class. \n",
    "\n",
    "The starting point for the development of `pywatershed` was the National Hydrologic Model (NHM, Regan et al., 2018) configuration of the Precipitation-Runoff Modeling System (PRMS, Regan et al., 2015). In this notebook, we'll first construct a full NHM configuration. The spatial domain we'll use will again be the Delaware River Basin. Once we construct the full NHM, we'll look at how we can also construct sub-models of the NHM.\n",
    "\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d5fb06-f73f-489b-9eb8-760e7f5fee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib as pl\n",
    "import pydoc\n",
    "from copy import deepcopy\n",
    "from platform import processor\n",
    "from pprint import pprint\n",
    "from sys import platform\n",
    "\n",
    "import hvplot.xarray  # noqa\n",
    "import jupyter_black\n",
    "import numpy as np\n",
    "import pywatershed as pws\n",
    "import xarray as xr\n",
    "import yaml\n",
    "from helpers import do_not_run_this_cell\n",
    "from pywatershed.utils import gis_files\n",
    "from pywatershed.utils.path import dict_pl_to_str\n",
    "\n",
    "jupyter_black.load()  # auto-format the code in this notebook\n",
    "\n",
    "pws.utils.gis_files.download()  # this downloads GIS files\n",
    "\n",
    "pkg_root_dir = pws.constants.__pywatershed_root__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a424718e-8512-4e53-b0e6-587a20639d90",
   "metadata": {},
   "source": [
    "## Domain Plot to get to know the area\n",
    "\n",
    "Before diving in to pywatershed models, let's use one of its built-in tools to get familiar with the application domain. We'll combine the GIS files for the HRUs and the Segments in this domain with their parameters to learn more about how the model represents quantities in pyhiscal space. Please zoom in and out and select different layers. We aim to add more functionality to this plot over time, stay tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6a0f3d-4583-4a96-844d-bbbeb652d374",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_output_dir = pl.Path(\"./step2_multi-process_models\")\n",
    "\n",
    "domain_dir = pkg_root_dir / \"data/drb_2yr\"\n",
    "\n",
    "domain_gis_dir = pkg_root_dir / \"data/pywatershed_gis/drb_2yr\"\n",
    "shp_file_hru = domain_gis_dir / \"HRU_subset.shp\"\n",
    "shp_file_seg = domain_gis_dir / \"Segments_subset.shp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68904b2-640e-4b92-8723-87ff94c3efef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_hru = pws.Parameters.from_netcdf(domain_dir / \"parameters_dis_hru.nc\")\n",
    "start_lat = dis_hru.parameters[\"hru_lat\"].mean()\n",
    "start_lon = dis_hru.parameters[\"hru_lon\"].mean()\n",
    "\n",
    "pws.plot.DomainPlot(\n",
    "    hru_shp_file=shp_file_hru,\n",
    "    segment_shp_file=shp_file_seg,\n",
    "    hru_parameters=domain_dir / \"parameters_dis_hru.nc\",\n",
    "    hru_parameter_names=[\n",
    "        \"nhm_id\",\n",
    "        \"hru_lat\",\n",
    "        \"hru_lon\",\n",
    "        \"hru_area\",\n",
    "    ],\n",
    "    segment_parameters=domain_dir / \"parameters_dis_seg.nc\",\n",
    "    segment_parameter_names=[\n",
    "        \"nhm_seg\",\n",
    "        \"seg_length\",\n",
    "        \"seg_slope\",\n",
    "        \"seg_cum_area\",\n",
    "    ],\n",
    "    start_lat=start_lat,\n",
    "    start_lon=start_lon,\n",
    "    start_zoom=7,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8896bb-96bc-4879-a2d5-38d083b33edd",
   "metadata": {},
   "source": [
    "## An NHM multi-process model for the Delaware River Basin\n",
    "The 8 conceptual `Process` classes that comprise the NHM are, in order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529247fe-7f0b-4783-90a4-a4ddf55c1489",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhm_processes = [\n",
    "    pws.PRMSSolarGeometry,\n",
    "    pws.PRMSAtmosphere,\n",
    "    pws.PRMSCanopy,\n",
    "    pws.PRMSSnow,\n",
    "    pws.PRMSRunoff,\n",
    "    pws.PRMSSoilzone,\n",
    "    pws.PRMSGroundwater,\n",
    "    pws.PRMSChannel,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdde067-5032-443d-9c7c-6fc0340f7be8",
   "metadata": {},
   "source": [
    "We'll use this list of classes shortly to construct the NHM.\n",
    "\n",
    "A multi-process model is assembled by the `Model` class. We can take a quick look at the first 21 lines of help on `Model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1bfbf7-00e0-4642-b7a6-dc06eee5722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is equivalent to help() but we get the multiline string and just look at part of it\n",
    "model_help = pydoc.render_doc(pws.Model, \"Help on %s\")\n",
    "# the first 22 lines of help(pws.Model)\n",
    "print(\"\\n\".join(model_help.splitlines()[0:22]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbfb5e1-adf4-4eef-949a-90476e00b9f2",
   "metadata": {},
   "source": [
    "The `help()` mentions that there are 2 distinct ways of instantiating a `Model` class. In this notebook, we focus on the pywatershed-centric instantiation and leave the PRMS-legacy instantiation for another time. \n",
    "\n",
    "With the pywatershed-centric approach, the first argument is a \"model dictionary\" which does nearly all the work (the other arguments will be their default values). The `help()` describes the model dictionary and provides examples. Please use it for reference and more details. Here we'll give an extended concrete example. The `help()` also describes how a `Model` can be instantiated from a model dictionary contained in a YAML file. First, we'll build a model dictionary in memory, then we'll write it out as a yaml file and instantiate our model directly from the YAML file. \n",
    "\n",
    "### Construct the model specification in memory\n",
    "Because our (pre-existing) parameter files (which come with `pywatershed`) and our `Process` classes are consistently named, we can begin to build the model dictionary quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bd5038-e0bd-4e8e-9020-00520f18b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {}\n",
    "\n",
    "for proc in nhm_processes:\n",
    "    # this is the class name\n",
    "    proc_name = proc.__name__\n",
    "    # the processes can have arbitrary names in the model_dict and\n",
    "    # an instance should not have capitalized name anyway (according to\n",
    "    # python convention), so rename from the class name\n",
    "    proc_rename = \"prms_\" + proc_name[4:].lower()\n",
    "    # each process has a dictionary of information\n",
    "    model_dict[proc_rename] = {}\n",
    "    # alias to shorten lines below\n",
    "    proc_dict = model_dict[proc_rename]\n",
    "    # required key \"class\" specifys the class\n",
    "    proc_dict[\"class\"] = proc\n",
    "    # the \"parameters\" key provides an instance of Parameters\n",
    "    proc_param_file = domain_dir / f\"parameters_{proc_name}.nc\"\n",
    "    proc_dict[\"parameters\"] = pws.Parameters.from_netcdf(proc_param_file)\n",
    "    # the \"dis\" key provides the name of the discretizations\n",
    "    # which we'll supply shortly to the model dictionary\n",
    "    if proc_rename == \"prms_channel\":\n",
    "        proc_dict[\"dis\"] = \"dis_both\"\n",
    "    else:\n",
    "        proc_dict[\"dis\"] = \"dis_hru\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f649ef-2723-44b9-9c49-3f2db85a1010",
   "metadata": {},
   "source": [
    "Let's look at what we have so far in the `model_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57647846-7159-4405-8e46-aab00594c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(model_dict, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059641b6-8fbb-49b4-a95f-4e7fd29af3a6",
   "metadata": {},
   "source": [
    "We have given a name to each process and then supplied the class, its parameters, and its discretization for the full set of processes. Now we'll need to add the discretizations to the model dictionary. They are added at the top level and correspond to the names the processes used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c45a89-b394-4f90-af7d-7bfcf715b0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = model_dict | {\n",
    "    \"dis_hru\": pws.Parameters.from_netcdf(\n",
    "        domain_dir / \"parameters_dis_hru.nc\"\n",
    "    ),\n",
    "    \"dis_both\": pws.Parameters.from_netcdf(\n",
    "        domain_dir / \"parameters_dis_both.nc\"\n",
    "    ),\n",
    "}\n",
    "pprint(model_dict, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75b11ab-4c2c-4671-9127-1578d8a9cfe0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "For the time being, `PRMSChannel` needs to know about both HRUs and segments, so `dis_both` is used. We plan to remove this requirement in the near future by implementing \"exchanges\" between processes into the model dictionary.\n",
    "\n",
    "You may have noticed that we are missing a `Control` object to provide time information to the processes. We'll create it and we'll also create a list of the order that the processes are executed.\n",
    "\n",
    "Though we have input available to run 2 years of simulation, we'll restrict the model run to the first 6 months for demonstration purposes. (Feel free to increase this to the full 2 years available, if you like.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4e38dc-f01c-4ffc-933e-d4c2a901a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir = nb_output_dir / \"run_dir\"\n",
    "control = pws.Control(\n",
    "    start_time=np.datetime64(\"1979-01-01T00:00:00\"),\n",
    "    end_time=np.datetime64(\"1979-07-01T00:00:00\"),\n",
    "    time_step=np.timedelta64(24, \"h\"),\n",
    "    options={\n",
    "        \"input_dir\": domain_dir,\n",
    "        \"budget_type\": \"error\",\n",
    "        \"netcdf_output_dir\": run_dir,\n",
    "    },\n",
    ")\n",
    "model_order = [\"prms_\" + proc.__name__[4:].lower() for proc in nhm_processes]\n",
    "model_dict = model_dict | {\"control\": control, \"model_order\": model_order}\n",
    "pprint(model_dict, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546a7ac8-c23e-4632-b655-6dc8e884172d",
   "metadata": {},
   "source": [
    "### Instantiate the model and view the ModelGraph\n",
    "\n",
    "The `model_dict` now specifies a complete model built from multiple processes. They way these processes are connected can be figured out by the `Model` class, because each process fully describes itself (as we saw in the previous notebook). If we instantiate a model from this `model_dict`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ba1ed4-fb1b-4fa1-bd38-f47113bdaa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pws.Model(model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7260278e-91ff-47af-a2c1-b9c02e8beaa1",
   "metadata": {},
   "source": [
    "we can examine how the `Processes` are all connected using the `ModelGraph` class. We'll bring in the default color scheme for NHM `Processes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c81929f-76d8-470f-9976-7d23f8c1af91",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "palette = pws.analysis.utils.colorbrewer.nhm_process_colors(model)\n",
    "pws.analysis.utils.colorbrewer.jupyter_palette(palette)\n",
    "show_params = not (platform == \"darwin\" and processor() == \"arm\")\n",
    "try:\n",
    "    pws.analysis.ModelGraph(\n",
    "        model,\n",
    "        hide_variables=False,\n",
    "        process_colors=palette,\n",
    "        show_params=show_params,\n",
    "    ).SVG(verbose=True, dpi=48)\n",
    "except:\n",
    "    static_url = \"https://github.com/EC-USGS/pywatershed/releases/download/1.1.0/notebook_01_cell_11_model_graph.png\"\n",
    "    print(\n",
    "        f\"Dot fails on some machines. You can see the graph at this url: {static_url}\"\n",
    "    )\n",
    "    from IPython.display import Image\n",
    "\n",
    "    display(Image(url=static_url, width=1300))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57ed289-17ba-4925-922d-d8e6f01bbb6b",
   "metadata": {},
   "source": [
    "### Questions\n",
    "* What are the inputs for this model and where are these found? Is there anything special about those files? Could we drive any process from file?\n",
    "* Can you see from where each process gets its inputs in this model? What is the largest number of other processes a single process draws inputs from?\n",
    "* Are some of the arrows 2-way?\n",
    "* Which processes are mass conservative? Can you see the terms involved in mass conservation?\n",
    "* Which process has the greatest/smallest ratio of number of parameters to number of variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc28c9b-8d33-479f-b4f8-b8985d86d9c8",
   "metadata": {},
   "source": [
    "### Run the model\n",
    "Now we'll initialize NetCDF output and run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707d6a26-3aa0-4064-b1ea-117fd2bff6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.run(finalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5cd00e-46a5-4778-abc5-f3566f7fe74a",
   "metadata": {},
   "source": [
    "Now we have a finalized run of our model. Before we look at the output of the run, note that the model specification in this example was constructed interactively in memory. We can also specify the model construction with a YAML file. This is shown in the notebook in the pywatershed repository [examples/01_multi-process_models.ipynb](https://github.com/EC-USGS/pywatershed/blob/develop/examples/01_multi-process_models.ipynb), on which this notebook is based. \n",
    "\n",
    "We can quite easily look at all the output resulting from our run by looking at the netcdf files in the run directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f305fd-0cfa-4b89-a1fb-c32e72490b8a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "output_files = sorted(run_dir.glob(\"*.nc\"))\n",
    "print(len(output_files))\n",
    "pprint(output_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a201df-6a88-4cb3-ada8-95b8f949de89",
   "metadata": {},
   "source": [
    "The following code will let us examine output variables, plotting the full timeseries at individual locations which can be scrolled through using the bar on the right side. It will not work to look at the out budget output files, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5e4070-6af5-4ff3-921e-fae61e3a8fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = \"albedo\"\n",
    "var_da = xr.open_dataarray(run_dir / f\"{var}.nc\")\n",
    "var_da.hvplot(groupby=var_da.dims[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3078346e-8b0f-48f0-b7a9-4200714b403b",
   "metadata": {},
   "source": [
    "We'll plot the last variable in the loop, `unused_potet`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45454b13-020b-4867-bd34-21c8cdb00834",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "proc_plot = pws.analysis.process_plot.ProcessPlot(\n",
    "    gis_files.gis_dir / \"drb_2yr\"\n",
    ")\n",
    "proc_classes = [model_dict[nn][\"class\"] for nn in model_order]\n",
    "\n",
    "\n",
    "def get_var_proc_class(var_name):\n",
    "    for proc_class in proc_classes:\n",
    "        if var_name in proc_class.get_variables():\n",
    "            return proc_class\n",
    "\n",
    "\n",
    "proc_plot.plot_hru_var(\n",
    "    var_name=var,\n",
    "    process=get_var_proc_class(var),\n",
    "    data=var_da.mean(dim=\"time\"),\n",
    "    data_units=var_da.attrs[\"units\"],\n",
    "    nhm_id=var_da[\"nhm_id\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97d3fff-4a83-4a4e-a22f-ed5da54412cd",
   "metadata": {},
   "source": [
    "We can also make a spatial plot of the streamflow using a transform for line width representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bc3067-9f56-4483-9ce6-d6fe8b1e0f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# var = \"seg_outflow\"\n",
    "# var_da = xr.open_dataarray(run_dir / f\"{var}.nc\")\n",
    "\n",
    "# def xform_width(vals):\n",
    "#     flow_log = np.maximum(np.log(vals + 1.0e-4), 0.0)\n",
    "#     width_max = 5\n",
    "#     width_min = 0.2\n",
    "#     flow_log_lw = (width_max - width_min) * (flow_log - np.min(flow_log)) / (\n",
    "#         np.max(flow_log) - np.min(flow_log)\n",
    "#     ) + width_min\n",
    "#     return flow_log_lw\n",
    "\n",
    "\n",
    "# proc_plot.plot(\n",
    "#     var,\n",
    "#     process=get_var_proc_class(var),\n",
    "#     value_transform=xform_width,\n",
    "#     data=var_da.mean(dim=\"time\"),\n",
    "#     title=f\"{var}\",\n",
    "#     aesthetic_width=True,\n",
    "# )\n",
    "\n",
    "# #proc_plot.plot(var_name, proc, title=var_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae49ade-3c8b-4fe8-af5a-9497f856151b",
   "metadata": {},
   "source": [
    "### Reduce model output to disk\n",
    "It's worth noting that quite a lot of output was written and that in many cases the amount of output can be reduced in favor of imporving/reducing model run time. In the next cell, we show how you would reduce the output by setting `control.options['netcdf_output_var_names]`. We'll suppose that we only want the output variables from the `PRMSGroundwater` and `PRMSChannel` processes. Note that we are just combining the variable names returned by these two processes' `.get_variables()` methods. However, we could specify any list of variable names we like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284d8184-9837-4c01-97db-ea1e53f956ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%do_not_run_this_cell\n",
    "desired_output = [\n",
    "    *pws.PRMSGroundwater.get_variables(),\n",
    "    *pws.PRMSChannel.get_variables(),\n",
    "]control_cp.options[\"netcdf_output_var_names\"] = desired_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b420c326-be66-41df-82f1-3c332c1f85ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "If I reduce the original ~150 output files to just those specified in the above cell, run time is about 60% on my Mac."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e3d4c4-8bea-44e2-be76-b274bf04c3d7",
   "metadata": {},
   "source": [
    "## NHM Submodel for the Delaware River Basin \n",
    "In many cases, running the full NHM model may not be necessary and it may be advantageous to just run some of the processes in it. Pywatershed gives you this flexibility. Suppose you wanted to change parameters or model process representation in the PRMSSoilzone to better predict streamflow. As the model is 1-way coupled, you can simply run a submodel starting with PRMSSoilzone and running through PRMSChannel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9e1807-bed4-4c52-99e0-b5a58e63eaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "submodel_processes = [pws.PRMSSoilzone, pws.PRMSGroundwater, pws.PRMSChannel]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd91adfe-a875-4873-b8ba-85778a7cb651",
   "metadata": {},
   "source": [
    "This prompts the question, what inputs/forcing data do we need for this submodel? We can ask each individual process for its inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbc5725-799e-435c-8fdd-86bce771bdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "submodel_input_dict = {\n",
    "    pp.__name__: pp.get_inputs() for pp in submodel_processes\n",
    "}\n",
    "pprint(submodel_input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002e6e21-c16d-404f-9910-e764f18215b9",
   "metadata": {},
   "source": [
    "And which inputs are supplied by variables within this submodel? We ask each process for its variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b97c7e-7d2e-4279-a7ee-b674679a180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submodel_vars_dict = {\n",
    "    pp.__name__: pp.get_variables() for pp in submodel_processes\n",
    "}\n",
    "pprint(submodel_vars_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed052fe-a46d-4470-8df5-bf03e2ecbb7d",
   "metadata": {},
   "source": [
    "We consolidate inputs and variables (each over all processes) and take a set difference of inputs and variables to know what inputs/forcings we need from file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99082da-cd5d-4c98-b57c-f716ebd01f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "submodel_inputs = set([ii for tt in submodel_input_dict.values() for ii in tt])\n",
    "submodel_variables = set(\n",
    "    [ii for tt in submodel_vars_dict.values() for ii in tt]\n",
    ")\n",
    "submodel_file_inputs = tuple(submodel_inputs - submodel_variables)\n",
    "pprint(submodel_file_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c789efb-b982-48d2-91cd-c555820fccb3",
   "metadata": {},
   "source": [
    "And where will we get these input files? You'll notice that these files do not come with the repository. Instead they are generated when we ran the full NHM model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b453372-b218-4936-9046-3b47cf64460b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "yaml_output_dir = pl.Path(control.options[\"netcdf_output_dir\"])\n",
    "for ii in submodel_file_inputs:\n",
    "    input_file = yaml_output_dir / f\"{ii}.nc\"\n",
    "    assert input_file.exists()\n",
    "    print(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e933dcd1-ce14-4113-b829-693e830e9172",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "Well, that was a lot of work. But, as alluded to above, the `Model` object does the above so you dont have to. You just learned something about how the flow of information between processes is enabled by the design and how one can query individual processes in `pywatershed`. But we could instantiate the submodel and plot this wiring up, just as we plotted the `ModelGraph` of the full model. We'll create the submodel in a new `run_dir` and we'll use outputs from the full model above as inputs to this submodel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ef25e0-7d30-446a-acfd-28af3d6e5ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir = pl.Path(nb_output_dir / \"nhm_sub\").resolve()\n",
    "run_dir.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "control_cp = deepcopy(control)\n",
    "# It is key that inputs exist from previous full-model run\n",
    "control_cp.options[\"input_dir\"] = yaml_output_dir.resolve()\n",
    "control_cp.options[\"netcdf_output_dir\"] = run_dir.resolve()\n",
    "control_yaml_file = run_dir / \"control.yaml\"\n",
    "control_cp.to_yaml(control_yaml_file)\n",
    "pprint(control.to_dict(), sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b81d5f5-e733-4347-b898-5e1a23f36a06",
   "metadata": {},
   "source": [
    "Now we will use the existing `model_dict` in memory, tayloring to the above and just keeping the processes of interest in the submodel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd108f6-aa1f-4f59-90c5-a8fe6018c90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict[\"control\"] = str(control_yaml_file)\n",
    "model_dict_yaml_file = run_dir / \"model_dict.yaml\"\n",
    "keep_procs = [\"prms_soilzone\", \"prms_groundwater\", \"prms_channel\"]\n",
    "model_dict[\"model_order\"] = keep_procs\n",
    "for kk in list(model_dict.keys()):\n",
    "    if isinstance(model_dict[kk], dict) and kk not in keep_procs:\n",
    "        del model_dict[kk]\n",
    "\n",
    "pprint(model_dict, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fdef0c-d77b-45bd-a6a2-cc227acbfae0",
   "metadata": {},
   "source": [
    "Now we write both the control and model dictionary to yaml files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4000d71a-3f58-46da-98fa-3c573b861b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_dict_yaml_file, \"w\") as file:\n",
    "    _ = yaml.dump(model_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f889c7-3763-4ed2-b93f-e3d6c2a3f67e",
   "metadata": {},
   "source": [
    "And finally we instantiate the submodel from the model dictionary yaml file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6570af-c1ae-4d1f-ad7b-1b32427fc647",
   "metadata": {},
   "outputs": [],
   "source": [
    "submodel = pws.Model.from_yaml(model_dict_yaml_file)\n",
    "submodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22155c95-1de6-45bf-9dea-f562df9fdeaf",
   "metadata": {},
   "source": [
    "Now to look at the `ModelGraph` for the submodel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cae1a7a-e66e-4816-942a-201edab68b72",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "show_params = not (platform == \"darwin\" and processor() == \"arm\")\n",
    "try:\n",
    "    pws.analysis.ModelGraph(\n",
    "        submodel,\n",
    "        hide_variables=False,\n",
    "        process_colors=palette,\n",
    "        show_params=show_params,\n",
    "    ).SVG(verbose=True, dpi=48)\n",
    "except:\n",
    "    static_url = \"https://github.com/EC-USGS/pywatershed/releases/download/1.1.0/notebook_01_cell_45_submodel_graph.png\"\n",
    "    print(\n",
    "        f\"Dot fails on some machines. You can see the graph at this url: {static_url}\"\n",
    "    )\n",
    "    from IPython.display import Image\n",
    "\n",
    "    display(Image(url=static_url, width=700))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f9c997-9937-4207-9ca9-102f2b34f499",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "Note that the required inputs to the submodel are quire different and rely on the existence of these files having already been output by the full model. \n",
    "\n",
    "Now we can initalize output and run the submodel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35002e93-b777-472e-8899-36548c1ea923",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "submodel.run(finalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c787a163-c4dd-4826-b0f2-73e1b006c081",
   "metadata": {},
   "source": [
    "We'll, that saved us some time. The run is similar to before, just using fewer processes. \n",
    "\n",
    "The final time is still in memory. We can take a look at, say, recharge. Before plotting, let's take a look at the data and the metadata for recharge a bit closer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbf15be-871b-4b58-ad1f-45bfcb37fedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(pws.meta.find_variables(\"recharge\"))\n",
    "print(\n",
    "    \"PRMSSoilzone dimension names: \",\n",
    "    submodel.processes[\"prms_soilzone\"].dimensions,\n",
    ")\n",
    "print(\"nhru: \", submodel.processes[\"prms_soilzone\"].nhru)\n",
    "print(\n",
    "    \"PRMSSoilzone recharge shape: \",\n",
    "    submodel.processes[\"prms_soilzone\"][\"recharge\"].shape,\n",
    ")\n",
    "print(\n",
    "    \"PRMSSoilzone recharge type: \",\n",
    "    type(submodel.processes[\"prms_soilzone\"][\"recharge\"]),\n",
    ")\n",
    "print(\n",
    "    \"PRMSSoilzone recharge dtype: \",\n",
    "    submodel.processes[\"prms_soilzone\"][\"recharge\"].dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af149db1-970c-47d2-b9dd-0d0b21f80810",
   "metadata": {},
   "source": [
    "First we access the metadata on `recharge` and we see its description, dimension, type, and units. The we look at the dimension names of the PRMSSoilzone process in whith it is found. We see the length of the `nhru` dimension and that this is the only dimension on `recharge`. We also see that `recharge` is a `numpy.ndarray` with data type `float64`.\n",
    "\n",
    "So recharge only has spatial dimension. It is written to file with each timestep (or periodically). However, the last timestep is still in memory (even though we've finalized the run) and we can visualize it. The data are on the unstructured/polygon grid of Hydrologic Response Units (HRUs), we'll visualize the spatial distribution at this final time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac5832-7437-467e-901d-c40e2d9ddab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_plot = pws.analysis.process_plot.ProcessPlot(\n",
    "    gis_files.gis_dir / \"drb_2yr\"\n",
    ")\n",
    "proc_name = \"prms_soilzone\"\n",
    "var_name = \"ssr_to_gw\"\n",
    "proc = submodel.processes[proc_name]\n",
    "display(proc_plot.plot(var_name, proc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13500c68-c363-4f8b-8db0-2c069aa4c5d7",
   "metadata": {},
   "source": [
    "We can easily check the results of our submodel model against our full model. This gives us an opportunity to look at the output files. We can start with recharge as our variable of interest. The model NetCDF output can be read in using `xarray` where we can see all the relevant metadata quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921eb7ef-c6e1-4ecf-b475-59d8552117b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = \"recharge\"\n",
    "nhm_da = xr.open_dataarray(yaml_output_dir / f\"{var}.nc\")\n",
    "sub_da = xr.open_dataarray(run_dir / f\"{var}.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e7b2e0-c9b6-4100-a328-42add2cbc4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(nhm_da)\n",
    "display(sub_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f317363-c765-4dda-a972-92f5265abd47",
   "metadata": {},
   "source": [
    "Now we can compare all output variables common to both runs, asserting that the two runs gave equal output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfeb46f-8d29-49f4-8945-f9b443831f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in submodel_variables:\n",
    "    nhm_da = xr.open_dataarray(yaml_output_dir / f\"{var}.nc\")\n",
    "    sub_da = xr.open_dataarray(run_dir / f\"{var}.nc\")\n",
    "    xr.testing.assert_equal(nhm_da, sub_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7ed710-d23a-4c80-8b28-0a1612a636b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_name = \"dprst_seep_hru\"\n",
    "nhm_da = xr.open_dataarray(yaml_output_dir / f\"{var_name}.nc\")\n",
    "sub_da = xr.open_dataarray(run_dir / f\"{var_name}.nc\")\n",
    "scat = xr.merge(\n",
    "    [nhm_da.rename(f\"{var_name}_yaml\"), sub_da.rename(f\"{var_name}_subset\")]\n",
    ")\n",
    "\n",
    "display(\n",
    "    scat.hvplot(\n",
    "        x=f\"{var_name}_yaml\", y=f\"{var_name}_subset\", groupby=\"nhm_id\"\n",
    "    ).opts(data_aspect=1)\n",
    ")\n",
    "\n",
    "scat.hvplot(y=f\"{var_name}_subset\", groupby=\"nhm_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91401478-34ab-4277-804e-6ee2d4403d52",
   "metadata": {},
   "source": [
    "## References\n",
    "* Regan, R. S., Markstrom, S. L., Hay, L. E., Viger, R. J., Norton, P. A., Driscoll, J. M., & LaFontaine, J. H. (2018). Description of the national hydrologic model for use with the precipitation-runoff modeling system (prms) (No. 6-B9). US Geological Survey.\n",
    "* Regan, R.S., Markstrom, S.L., LaFontaine, J.H., 2022, PRMS version 5.2.1: Precipitation-Runoff Modeling System (PRMS): U.S. Geological Survey Software Release, 02/10/2022."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
